{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swalehaparvin/Working_with_LLMs/blob/main/HF_Bias_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3KD3WXU3l-O"
      },
      "source": [
        "# Evaluating Bias and Toxicity in Language Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAscNNUD3l-P"
      },
      "source": [
        "In this notebook, we'll see how to evaluate different aspects of bias and toxicity of large language models hosted on [ðŸ¤— Transformers](https://github.com/huggingface/transformers). We will cover three types of bias evaluation, which are:\n",
        "\n",
        "* **Toxicity**: aims to quantify the toxicity of the input texts using a pretrained hate speech classification model.\n",
        "\n",
        "* **Regard**: returns the estimated language polarity towards and social perceptions of a demographic (e.g. gender, race, sexual orientation).\n",
        "\n",
        "* **HONEST score**: measures hurtful sentence completions based on multilingual hate lexicons.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhYeLeHeC9yq"
      },
      "source": [
        "The workflow of the evaluations described above is the following:\n",
        "\n",
        "* Choosing a language model for evaluation (either from the [ðŸ¤— Hub](https://github.com/huggingface/models) or by training your own\n",
        "* Prompting the model with a set of predefined prompts\n",
        "* Running the resulting generations through the relevant metric or measurement to evaluate its bias.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4cRE8IbIrIV"
      },
      "source": [
        "First things first: you need to install ðŸ¤— Transformers, Datasets and Evaluate!\n",
        "\n",
        "If you're opening this notebook locally, make sure your environment has an install from the last version of those libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers evaluate -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r_n9OWV3l-Q"
      },
      "source": [
        "## Choosing a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kswRMhPc3l-Q"
      },
      "source": [
        "The steps describe above depend on being able to *prompt* your model in order to evaluate its *generations*. This means that the model has to be capable of text generation.\n",
        "\n",
        "You can consult all of the models on the ðŸ¤— Hub that are capable of this [here](https://huggingface.co/models?pipeline_tag=text-generation).\n",
        "\n",
        "We will prompt [GPT-2](https://huggingface.co/gpt2), one of the most popular models on the Hub:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jc3ky1_Iioe"
      },
      "source": [
        "## Evaluating Toxicity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyNSHBiTVSPU"
      },
      "source": [
        "Evaluating the toxicity of a model can help understand its behavior in general, or towards specific identity terms and groups.\n",
        "This can be done by prompting the model with a set of prompts and then evaluating the toxicity of its generations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTY1p--0KZ6p"
      },
      "source": [
        "### Loading Real Toxicity Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1-9jepM3l-W"
      },
      "source": [
        "To evaluate the toxicity of GPT-2 (or the model of your choice, we will use [Real Toxicity Prompts](https://huggingface.co/datasets/allenai/real-toxicity-prompts), a dataset of prompts created specifically for this purpose.\n",
        "It contains 100,000 (!) prompts that are stratified across 4 levels of toxicity, from least to most toxic, curated to better understand the risks of toxic degeneration in language models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPlvWCQlJoBK"
      },
      "source": [
        "Let's load the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "toxicity_prompts = load_dataset(\"allenai/real-toxicity-prompts\", split=\"train\")"
      ],
      "metadata": {
        "id": "LfQkGLfIyhkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHUmphG3IrI3"
      },
      "source": [
        "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXimAd9mJq1M"
      },
      "outputs": [],
      "source": [
        "toxicity_prompts[10]['prompt']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdRQHSJ3JTZ7"
      },
      "source": [
        "As you can see, each prompt in the dataset contains a dictionary which includes, among other things, the `text` of the prompts as well as its toxicity score from different aspects, based on the [Jigsaw Perspective API](https://perspectiveapi.com/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1q88rXvtLrSJ"
      },
      "source": [
        "For the sake of efficiency, we will pick a sample of 100 prompts from the total 100,000.\n",
        "Feel free to skip this step if you want to prompt your model with the whole dataset (but keep in mind that this can take an hour or more)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-H5yh3MM5P2"
      },
      "outputs": [],
      "source": [
        "def get_random_prompts(dataset, num_examples=100):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "    return(dataset[picks])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIRFxh0YNGEU"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "toxic_sample= get_random_prompts(toxicity_prompts)\n",
        "toxic_prompts = [p['text'] for p in toxic_sample['prompt']]\n",
        "print(toxic_prompts[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX8ciyVWKiuO"
      },
      "source": [
        "### Prompting the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur5sNUcZ3l-g"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "text_generation = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izBpIIDaM4i_"
      },
      "source": [
        "\n",
        "\n",
        "We will feed the text of each of these prompts to our model, take the generation and remove the original prompt text, and store its generations in a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Uk8NROQ3l-k"
      },
      "outputs": [],
      "source": [
        "model_continuations=[]\n",
        "for prompt in toxic_prompts:\n",
        "  generation = text_generation(prompt, max_length=50, do_sample=False, pad_token_id=50256)\n",
        "  continuation = generation[0]['generated_text'].replace(prompt,'')\n",
        "  model_continuations.append(continuation)\n",
        "\n",
        "print('Generated '+ str(len(model_continuations))+ ' continuations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNnWbywrQmUH"
      },
      "source": [
        "Let's print out a prompt and a continuation to see what kind of content our model generates:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LstiulTsQuEF"
      },
      "outputs": [],
      "source": [
        "toxic_prompts[40:45]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETlzv_ReQeNJ"
      },
      "outputs": [],
      "source": [
        "model_continuations[40:45]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKerdF353l-o"
      },
      "source": [
        "As we can see, depending on the prompt, the output of the model can vary widely!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhWVsORuRALq"
      },
      "source": [
        "### Evaluating toxicity of model outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj5buPivRlcd"
      },
      "source": [
        "Now let's load the toxicity evaluation measurement!\n",
        "The default model used is [roberta-hate-speech-dynabench-r4](https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target).\n",
        "In this model, â€˜hateâ€™ is defined as \"abusive speech targeting specific group characteristics, such as ethnic origin, religion, gender, or sexual orientation\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9en0SOhRaGz"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "toxicity = evaluate.load(\"toxicity\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDVeTzIbSXWm"
      },
      "source": [
        "Now let's run the model continuations through the measurement.\n",
        "\n",
        "We can look at different aspects of toxicity, for instance the ratio of toxic continuations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX-HAkm2RaOW"
      },
      "outputs": [],
      "source": [
        "toxicity_ratio = toxicity.compute(predictions=model_continuations, aggregation=\"ratio\")\n",
        "print(toxicity_ratio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oik5WxeLS7Vq"
      },
      "source": [
        "We can also look at the maximum toxicity of any continuation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8rQq6CcSjLP"
      },
      "outputs": [],
      "source": [
        "max_toxicity = toxicity.compute(predictions=model_continuations, aggregation=\"maximum\")\n",
        "print(max_toxicity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaIj4DjWTDMR"
      },
      "source": [
        "If you want to look at the toxicity of each individual continuation, you can `zip` through the continuation texts and the scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SATlN5_3TDVA"
      },
      "outputs": [],
      "source": [
        "tox_dict= {}\n",
        "all_toxicity = toxicity.compute(predictions=model_continuations)\n",
        "for text, score in zip(model_continuations, all_toxicity['toxicity']):\n",
        "  tox_dict[text] = score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ogr6MecUVMl"
      },
      "source": [
        " Then we can also `sort` by toxicity score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sq7IY09lUVS3"
      },
      "outputs": [],
      "source": [
        "tox_dict = (dict(sorted(tox_dict.items(), key=lambda item: item[1], reverse=True)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaOEQnU9Udfs"
      },
      "source": [
        "Feel free to explore the top toxic continuations of the model like so:\n",
        "\n",
        "\n",
        "```\n",
        "list(tox_dict.keys())[0]\n",
        "```\n",
        "\n",
        "**CW: Many of model continuations may contain terms related to sexuality, violence, and/or hate speech**!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEA1ju653l-p"
      },
      "source": [
        "## Evaluating Regard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw8T4xveVlmO"
      },
      "source": [
        "Regard is a measurement that aims to evaluate language polarity towards and social perceptions of a demographic (e.g. gender, race, sexual orientation). It was first proposed in a [2019 paper by Sheng et al.](https://arxiv.org/pdf/1909.01326.pdf) specifically as a measure of bias towards a demographic.\n",
        "\n",
        "We will therefore prompt our model with prompts about different identity groups to evaluate how the continuations is produces differ between them using regard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0egrCyYDWig9"
      },
      "source": [
        "### Loading BOLD prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-5Ymt5dW8Br"
      },
      "source": [
        "We will be using the [BOLD dataset](https://huggingface.co/datasets/AlexaAI/bold), which was created to evaluate fairness in open-ended language generation.\n",
        "It consists of 23,679 different text generation prompts that allow fairness measurement across five domains: profession, gender, race, religious ideologies, and political ideologies.\n",
        "\n",
        "We will be working with a sample from the gender domain in the current tutorial, but feel free to explore other domains!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WGBCO343l-q"
      },
      "outputs": [],
      "source": [
        "bold = load_dataset(\"AlexaAI/bold\", split=\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6UbnNrzX1gV"
      },
      "source": [
        "Since the gender domain contains 2363 prompts, we will sample 100 of them (50 female and 50 male) to test our model.\n",
        "Once again, feel free to use the whole domain (and even the whole prompt dataset) if you have the time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJSr6lLlXpd8"
      },
      "outputs": [],
      "source": [
        "from random import sample\n",
        "female_bold = (sample([p for p in bold if p['category'] == 'American_actresses'],50))\n",
        "male_bold = (sample([p for p in bold if p['category'] == 'American_actors'],50))\n",
        "female_bold[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RNBAMM0aFD5"
      },
      "source": [
        "We will keep only the first prompt for each American actor and actress, to get 50 prompts for each category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-yxh7qXZpXl"
      },
      "outputs": [],
      "source": [
        "male_prompts = [p['prompts'][0] for p in male_bold]\n",
        "female_prompts = [p['prompts'][0] for p in female_bold]\n",
        "male_prompts[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a12iXnZia4qp"
      },
      "source": [
        "## Prompting our Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ2AtHJna-Tw"
      },
      "source": [
        "We will use the `text_generation` pipeline defined above, with the same model, this time prompting the model with the male- and female- category prompts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6JuJj5oa43a"
      },
      "outputs": [],
      "source": [
        "male_continuations=[]\n",
        "for prompt in male_prompts:\n",
        "  generation = text_generation(prompt, max_length=50, do_sample=False, pad_token_id=50256)\n",
        "  continuation = generation[0]['generated_text'].replace(prompt,'')\n",
        "  male_continuations.append(continuation)\n",
        "\n",
        "print('Generated '+ str(len(male_continuations))+ ' male continuations')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYE3Y1lIa46P"
      },
      "outputs": [],
      "source": [
        "female_continuations=[]\n",
        "for prompt in female_prompts:\n",
        "  generation = text_generation(prompt, max_length=50, do_sample=False, pad_token_id=50256)\n",
        "  continuation = generation[0]['generated_text'].replace(prompt,'')\n",
        "  female_continuations.append(continuation)\n",
        "\n",
        "print('Generated '+ str(len(female_continuations))+ ' female continuations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16GUe7Noc87T"
      },
      "source": [
        "Let's spot check some male and female prompts and continuations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RC5-Iv0Oc9CV"
      },
      "outputs": [],
      "source": [
        "print(male_prompts[42])\n",
        "print(male_continuations[42])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7-rl7nbdKhM"
      },
      "outputs": [],
      "source": [
        "print(female_prompts[42])\n",
        "print(female_continuations[42])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpOiBrJ13l-y"
      },
      "source": [
        "### Calculating Regard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiNdVyWWWouH"
      },
      "source": [
        "Let's load the regard metric and apply it to evaluate the bias of the two sets of continuations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v14nWmAXeTto"
      },
      "outputs": [],
      "source": [
        "regard = evaluate.load('regard', 'compare')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9xVAa3s3l-2"
      },
      "source": [
        "Now let's look at the difference between the two genders:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lYyG8dTguAL"
      },
      "outputs": [],
      "source": [
        "regard.compute(data = male_continuations, references= female_continuations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWMARyxiZk9C"
      },
      "source": [
        "We can see that male continuations are actually slightly less positive than female ones, with a -7% difference in positive regard, and a +8% difference in negative regard.\n",
        "We can look at the average regard for each category (negative, positive, neutral, other) for each group by using the `aggregation='average'` option:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkCrxRBJZdem"
      },
      "outputs": [],
      "source": [
        "regard.compute(data = male_continuations, references= female_continuations, aggregation = 'average')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKsnvYt_aZ7E"
      },
      "source": [
        "It's interesting to observe that given this sample of BOLD prompts and the GPT-2 model, female-prompted continuations are slightly more positive than male ones.\n",
        "\n",
        "You can try other categories of the BOLD dataset, e.g. race, profession, and religious and political ideologies to see how the model's bias towards different groups differs!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cdw6OmNPbP8v"
      },
      "source": [
        "## HONEST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGSOWHa7bY1q"
      },
      "source": [
        "The final bias evaluation approach that we will be looking at is HONEST, an approach that leverages prompts and evaluates model completions in order to measure hurtful sentence completions in language models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7Ln6eg3d8Wv"
      },
      "source": [
        "### Loading HONEST prompts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc23t-ofd-sH"
      },
      "source": [
        "We will be leveraging the [HONEST dataset](https://huggingface.co/datasets/MilaNLProc/honest), which provides a set of templates for measuring hurtful sentence completions in language models.\n",
        "The templates are provided in six languages (English, Italian, French, Portuguese, Romanian, and Spanish) for binary gender and in English for LGBTQAI+ individuals.\n",
        "\n",
        "**TW: This dataset contains content that are offensive and/or hateful in nature.**\n",
        "\n",
        "For the purposes of this analysis, we will use the `en_queer_nonqueer` subset of the dataset -- i.e. the part of the dataset that has prompts regarding LGBTQIA+ individuals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wFFpELrCFMP"
      },
      "outputs": [],
      "source": [
        "honest_dataset = load_dataset(\"MilaNLProc/honest\", 'en_queer_nonqueer', split='honest')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_vIfADndGzc"
      },
      "source": [
        "Let's take a look at the prompt categories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4VX6gWrcx5m"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "categories= [h['category'] for h in honest_dataset]\n",
        "Counter(categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS3zJhusdXOR"
      },
      "source": [
        "For the purposes of brevity, we will sample 50 from both the 'queer' and 'nonqueer' categories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPDtFR1HGwEd"
      },
      "outputs": [],
      "source": [
        "queer_prompts= sample([h['template_masked'].replace(' [M].','') for h in honest_dataset if h['category'].startswith('queer')], 50)\n",
        "nonqueer_prompts= sample([h['template_masked'].replace(' [M].','') for h in honest_dataset if h['category'].startswith('nonqueer')], 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXI_UetzeLLL"
      },
      "source": [
        "Let's spot-check a few:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HJV9JDCeLU1"
      },
      "outputs": [],
      "source": [
        "print(queer_prompts[42])\n",
        "print(nonqueer_prompts[42])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0tFnYFkF8rz"
      },
      "source": [
        "### Prompting the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5vKuwl_GBDG"
      },
      "source": [
        "We will use the `text_generation` pipeline (defined for the BOLD prompts above) to complete the templates of the HONEST dataset.\n",
        "\n",
        "We will also keep track of the category that the prompt corresponds to, which we will use for calculating the HONEST score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE51Cp-RKE9Q"
      },
      "source": [
        "In the [original HONEST paper](https://aclanthology.org/2021.naacl-main.191/), they look at the top 20 completions per prompt, which is what we will do below, defining `num_return_sequences` equal to 20. You can also change this number to have more or less generations per prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrPWc4eEGNjI"
      },
      "outputs": [],
      "source": [
        "k = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kesm0vUUFbwb"
      },
      "outputs": [],
      "source": [
        "queer_continuations=[]\n",
        "for prompt in queer_prompts:\n",
        "  generation = text_generation(prompt, max_length=len(tokenizer(prompt)['input_ids'])+10, num_return_sequences=k, pad_token_id=50256)\n",
        "  continuation = generation[0]['generated_text'].replace(prompt,'')\n",
        "  queer_continuations.append(continuation)\n",
        "\n",
        "print('Generated '+ str(len(queer_continuations))+ ' continuations')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwtquK6xHuUw"
      },
      "outputs": [],
      "source": [
        "nonqueer_continuations=[]\n",
        "for prompt in nonqueer_prompts:\n",
        "  generation = text_generation(prompt, max_length=len(tokenizer(prompt)['input_ids'])+10, num_return_sequences=k, pad_token_id=50256)\n",
        "  continuation = generation[0]['generated_text'].replace(prompt,'')\n",
        "  nonqueer_continuations.append(continuation)\n",
        "\n",
        "print('Generated '+ str(len(nonqueer_continuations))+ ' continuations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEFomIOAgOMR"
      },
      "source": [
        "### Calculating the HONEST score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUOk_c8Vg-J_"
      },
      "source": [
        "Now let's leverage the HONEST score to quantify the bias of the model based on these prompts:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(You will need to install `unidecode` if you haven't already)"
      ],
      "metadata": {
        "id": "kxXxg9HZCbNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "metadata": {
        "id": "wcZAy5swCOO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4NWS93rgO4I"
      },
      "outputs": [],
      "source": [
        "honest = evaluate.load('/content/evaluate/measurements/honest', 'en')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVJeiaMvi1Od"
      },
      "source": [
        "In order to leverage the comparison functionality of HONEST, we will need to define the groups that each of the continuations belong to, and concatenate the two lists together, splitting each word in the continuations using the `split()` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3K0QQDxbi0Gj"
      },
      "outputs": [],
      "source": [
        "groups = ['queer'] * 50 + ['nonqueer'] * 50\n",
        "continuations = [c.split() for c in queer_continuations] + [q.split() for q in nonqueer_continuations]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDgro5jYi0KL"
      },
      "outputs": [],
      "source": [
        "honest_score = honest.compute(predictions=continuations, groups = groups)\n",
        "print(honest_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the HONEST score for GPT-2 is actually the same for both categories! That would indicate that the model does not, on average, produce more hurtful completions towards queer versus non-queer categories.\n",
        "\n",
        "You can also try calculating the score for all of the prompts from the dataset, or explore the binary gender prompts (by reloading the dataset with `honest_dataset = load_dataset(\"MilaNLProc/honest\", 'en_binary', split='honest')`\n"
      ],
      "metadata": {
        "id": "J-LfE9Gnqb4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We hope that you enjoyed this tutorial for bias evaluation using ðŸ¤— Datasets, Transformers and Evaluate!\n",
        "\n",
        "#### Stay tuned for more bias metrics and measurements, as well as other tools for evaluating bias and fairness."
      ],
      "metadata": {
        "id": "fttGCa7FrB6t"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}