{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/744u2mw1Oixw5GmfZRJe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swalehaparvin/Working_with_LLMs/blob/main/Toxicity_in_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers evaluate -q\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "toxicity = evaluate.load(\"toxicity\", 'DaNLP/da-electra-hatespeech-detection', module_type=\"measurement\",)\n",
        "\n",
        "user_1 = [\"This is a test sentence.\", \"Another sentence for user 1.\"]\n",
        "user_2 = [\"This is a different test sentence.\", \"Sentence for user 2.\"]\n",
        "\n",
        "# Load the toxic classifier to inspect its labels\n",
        "toxic_classifier = AutoModelForSequenceClassification.from_pretrained('DaNLP/da-electra-hatespeech-detection')\n",
        "\n",
        "# Get the supported labels from the model's config\n",
        "model_labels = toxic_classifier.config.id2label\n",
        "print(\"Model labels:\", model_labels)\n",
        "\n",
        "# Based on the output of model_labels, choose the appropriate toxic_label.\n",
        "# Assuming 'toxic' is one of the keys, we'll use that.\n",
        "# Replace 'toxic' with the actual key if it's different in your model's labels.\n",
        "# Let's assume the key is 'toxic' based on the metric name, but you should verify from the print output.\n",
        "# If the key is an integer, you might need to find the corresponding string value.\n",
        "toxic_label_key = None\n",
        "for key, value in model_labels.items():\n",
        "    if value.lower() == 'toxic': # Or check for other relevant terms like 'offensive' or 'abusive'\n",
        "        toxic_label_key = value\n",
        "        break\n",
        "    # If 'toxic' is not found, try to find the label related to the DaNLP model's specific task,\n",
        "    # which is \"hatespeech detection\". We'll look for 'offensive', 'hate', or similar.\n",
        "    if 'offensive' in value.lower():\n",
        "         toxic_label_key = value\n",
        "         break\n",
        "    if 'hate' in value.lower():\n",
        "        toxic_label_key = value\n",
        "        break\n",
        "\n",
        "# If after checking, no suitable label is found, you might need to investigate the model's documentation\n",
        "# or the metric's documentation further. For now, we'll assume 'offensive' or similar is present.\n",
        "# As a fallback, let's check if there's a label at index 1, as toxic labels are often indexed after the non-toxic ones.\n",
        "if toxic_label_key is None and 1 in model_labels:\n",
        "    toxic_label_key = model_labels[1]\n",
        "    print(f\"Warning: Could not find 'toxic', 'offensive', or 'hate' in labels. Using label at index 1: {toxic_label_key}\")\n",
        "elif toxic_label_key is None:\n",
        "     raise ValueError(\"Could not determine the correct toxic_label from the model's configuration.\")\n",
        "\n",
        "\n",
        "# Calculate the individual toxicities using the determined toxic_label\n",
        "toxicity_1 = toxicity.compute(predictions=user_1, toxic_label=toxic_label_key)\n",
        "toxicity_2 = toxicity.compute(predictions=user_2, toxic_label=toxic_label_key)\n",
        "print(\"Toxicities (user_1):\", toxicity_1['toxicity'])\n",
        "print(\"Toxicities (user_2): \", toxicity_2['toxicity'])\n",
        "\n",
        "# Calculate the maximum toxicities using the determined toxic_label\n",
        "toxicity_1_max = toxicity.compute(predictions=user_1, aggregation=\"maximum\", toxic_label=toxic_label_key)\n",
        "toxicity_2_max = toxicity.compute(predictions=user_2, aggregation=\"maximum\", toxic_label=toxic_label_key)\n",
        "print(\"Maximum toxicity (user_1):\", toxicity_1_max['max_toxicity'])\n",
        "print(\"Maximum toxicity (user_2): \", toxicity_2_max['max_toxicity'])\n",
        "\n",
        "# Calculate the toxicity ratios using the determined toxic_label\n",
        "toxicity_1_ratio = toxicity.compute(predictions=user_1, aggregation=\"ratio\", toxic_label=toxic_label_key)\n",
        "toxicity_2_ratio = toxicity.compute(predictions=user_2, aggregation=\"ratio\", toxic_label=toxic_label_key)\n",
        "print(\"Toxicity ratio (user_1):\", toxicity_1_ratio['toxicity_ratio'])\n",
        "print(\"Toxicity ratio (user_2): \", toxicity_2_ratio['toxicity_ratio'])"
      ],
      "metadata": {
        "id": "B-ZtDeJTVaJU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}